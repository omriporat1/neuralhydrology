import os
import sys
import logging
import pandas as pd
import geopandas as gpd
import numpy as np
import xarray as xr
from rasterstats import zonal_stats
from datetime import datetime
import concurrent.futures
import rasterio

def setup_logging(log_file=None):
    if log_file:
        logging.basicConfig(filename=log_file, level=logging.INFO, format='%(asctime)s %(levelname)s: %(message)s')
    else:
        logging.basicConfig(level=logging.INFO, format='%(asctime)s %(levelname)s: %(message)s')

def format_date(dt):
    # dt: np.datetime64 or pd.Timestamp
    if isinstance(dt, np.datetime64):
        dt = pd.to_datetime(str(dt))
    return dt.strftime('%d/%m/%Y %H:%M')

def extract_basin_stats_for_timestep(args):
    # For parallelization: (basin_idx, basin_geom, rain_grid, gauge_grid, x_coords, y_coords, time, basin_id, log_dir)
    (basin_idx, basin_geom, rain_grid, gauge_grid, x_coords, y_coords, time, basin_id, log_dir) = args
    try:
        # Prepare a temporary raster for this timestep
        affine = rasterio.transform.from_bounds(
            x_coords[0], y_coords[0], x_coords[-1], y_coords[-1], rain_grid.shape[1], rain_grid.shape[0])
        zs_rain = zonal_stats(
            [basin_geom], rain_grid, affine=affine, stats=["mean", "max"], nodata=np.nan, all_touched=True)
        zs_gauges = zonal_stats(
            [basin_geom], gauge_grid, affine=affine, stats=["min", "max"], nodata=np.nan, all_touched=True)
        result = {
            'date': format_date(time),
            'mean_rain': zs_rain[0]['mean'],
            'max_rain': zs_rain[0]['max'],
            'min_gauges': zs_gauges[0]['min'],
            'max_gauges': zs_gauges[0]['max']
        }
        return (basin_id, result)
    except Exception as e:
        if log_dir:
            with open(os.path.join(log_dir, f'basin_{basin_id}_error.log'), 'a') as f:
                f.write(f"Error at {format_date(time)}: {e}\n")
        return (basin_id, None)

def process_basin_timeseries(basin_idx, basin_row, ds, x_coords, y_coords, log_dir, out_dir, parallel=False):
    basin_id = basin_row['id'] if 'id' in basin_row else basin_idx
    basin_geom = basin_row['geometry']
    times = ds['time'].values
    rain = ds['rain'].values
    gauge_count = ds['gauge_count'].values
    results = []
    args_list = [
        (basin_idx, basin_geom, rain[i], gauge_count[i], x_coords, y_coords, times[i], basin_id, log_dir)
        for i in range(len(times))
    ]
    if parallel:
        with concurrent.futures.ThreadPoolExecutor() as executor:
            for basin_id, res in executor.map(extract_basin_stats_for_timestep, args_list):
                if res:
                    results.append(res)
    else:
        for args in args_list:
            basin_id, res = extract_basin_stats_for_timestep(args)
            if res:
                results.append(res)
    # Save to CSV
    df = pd.DataFrame(results)
    csv_path = os.path.join(out_dir, f"basin_{basin_id}.csv")
    df.to_csv(csv_path, index=False, encoding='utf-8')
    logging.info(f"Saved basin {basin_id} results to {csv_path}")

def main():
    import argparse

    parser = argparse.ArgumentParser(description="Extract mean/max rain and gauge stats for each basin from NetCDF rain surfaces.")
    parser.add_argument('--basins', type=str, required=True, help='Path to basins shapefile')
    parser.add_argument('--netcdf', type=str, required=True, help='Path to NetCDF file with rain and gauge_count')
    parser.add_argument('--out_dir', type=str, default='basin_stats_output', help='Output directory for CSVs')
    parser.add_argument('--log_dir', type=str, default='logs', help='Directory for log files')
    parser.add_argument('--parallel', action='store_true', help='Enable parallel processing (per basin)')
    parser.add_argument('--log_file', type=str, default=None, help='Log file path')
    parser.add_argument('--start_time', type=str, default=None, help='Start time (inclusive) in YYYY-MM-DD[ HH:MM] format')
    parser.add_argument('--end_time', type=str, default=None, help='End time (inclusive) in YYYY-MM-DD[ HH:MM] format')
    args = parser.parse_args()

    os.makedirs(args.out_dir, exist_ok=True)
    os.makedirs(args.log_dir, exist_ok=True)
    setup_logging(args.log_file)

    logging.info(f"Loading basins from {args.basins}")
    basins = gpd.read_file(args.basins)
    if basins.crs is None or basins.crs.to_epsg() != 2039:
        basins = basins.to_crs(epsg=2039)


    logging.info(f"Loading NetCDF from {args.netcdf}")
    ds = xr.open_dataset(args.netcdf)
    # Filter by time range if specified
    if args.start_time or args.end_time:
        time_index = ds['time']
        if args.start_time:
            start = np.datetime64(args.start_time)
        else:
            start = time_index.values[0]
        if args.end_time:
            end = np.datetime64(args.end_time)
        else:
            end = time_index.values[-1]
        ds = ds.sel(time=slice(start, end))
        logging.info(f"Filtered time range: {str(start)} to {str(end)}. {ds['time'].size} timesteps remain.")
    x_coords = ds['x'].values
    y_coords = ds['y'].values

    logging.info(f"Processing {len(basins)} basins...")
    basin_rows = list(basins.iterrows())
    if args.parallel:
        with concurrent.futures.ProcessPoolExecutor() as executor:
            futs = [
                executor.submit(process_basin_timeseries, idx, row, ds, x_coords, y_coords, args.log_dir, args.out_dir, False)
                for idx, row in basin_rows
            ]
            for fut in concurrent.futures.as_completed(futs):
                pass  # All logging is handled in the function
    else:
        for idx, row in basin_rows:
            process_basin_timeseries(idx, row, ds, x_coords, y_coords, args.log_dir, args.out_dir, False)

    logging.info("All basins processed.")

if __name__ == '__main__':
    main()
